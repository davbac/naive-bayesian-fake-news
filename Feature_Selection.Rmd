---
title: "Feature selection for Naive Bayes Classifier"
author: "Barbiero Lorenzo"
date: "`r Sys.Date()`"
output: 
  prettydoc::html_pretty:
        theme: cayman
        highlight: github
        math: katex
---

EXERCISE 1  

```{r}
library(tidyverse)
```

1.1)
Import data

```{r}
train <- read.csv('archive/train.csv', header=TRUE)
```

```{r}
df <- data.frame(train)

# Convert everything to lowercase, remove punctuation, and split into separate words
df <- df |> mutate(Text = tolower(Text)) |>
  mutate(Text = str_replace_all(Text, "[[:punct:]]", ""))
  

# Count the occurrences of each word for each label
word_counts <- df |>
  separate_rows(Text, sep = "\\s+") |>
  group_by(Labels, Text) |>
  summarise(count = n()) |>
  #filter(!is.na(count)) |>
  arrange(-count)

# Print the word counts
print(word_counts)
```
```{r}
# Reformat the word_counts data frame
reformatted_word_counts <- word_counts %>%
  pivot_wider(names_from = Labels, values_from = count, values_fill = 0)%>%
  mutate(Sum = rowSums(across(-Text))) |> arrange(-Sum)

colnames(reformatted_word_counts) <- c("Text","Two","Three","One","Five","Zero","Four", "Total")

# Print the reformatted word counts
print(reformatted_word_counts)
```

Before moving forward with proper feature selection we will preventively remove two categories of words:
1) the ones too rare (e.g. the bottom 60%)

```{r}
# Minimum count threshold
min_count <- 5

# Filter out rows with counts below the threshold
filtered_word_counts <- reformatted_word_counts %>%
  filter(Total > min_count) 

# Print the final filtered word counts
print(filtered_word_counts)
```

At this point proper feature selection can start, we'll implement both mutual information and chi squared

Mutual function
```{r}
mutual_info <- function(N_11,N_01,N_10,N_00){
  N <- N_00+N_01+N_10+N_11
  N_1. <- N_01+N_11
  N_.1 <- N_10+N_11
  N_0. <- N_00+N_10
  N_.0 <- N_01+N_00
  
  info <- (N_11/N)*log2(N*N_11/(N_1.*N_.1)) + 
                   (N_10/N)*log2(N*N_10/(N_0.*N_.1)) + 
                   (N_01/N)*log2(N*N_01/(N_1.*N_.0)) +
                   (N_00/N)*log2(N*N_00/(N_0.*N_.0))

  return(info)
}
```

```{r}
test <- mutual_info(N_11=49,N_01 = 27652,N_10 = 141,N_00 = 774106)
print(test)
```


Chi squared
```{r}
chi2 <- function(N_11,N_01,N_10,N_00){
  N <- sum(N_00,N_10,N_11,N_01)
  E_11 <- (N_11+N_10)*(N_11+N_01)/N
  E_01 <- (N_01+N_11)*(N_01+N_00)/N
  E_10 <- (N_10+N_11)*(N_10+N_00)/N
  E_00 <- (N_00+N_10)*(N_00+N_01)/N
  
  chi <- sum((N_11-E_11)**2/E_11,(N_10-E_10)**2/E_10,(N_01-E_01)**2/E_01,(N_00-E_00)**2/E_00)
  
 return(chi) 
}
```

Iterate function

```{r}
# Create a Vector with Columns
columns = c("Text","Two","Three","One","Five","Zero","Four")
col_order = c("Text","Two","Three","One","Five","Zero","Four")

#Create a Empty DataFrame with 0 rows and n columns
results = data.frame(matrix(nrow = nrow(filtered_word_counts), ncol = length(columns))) 

# Assign column names
colnames(results) = columns

# Add word column
results$Text <- filtered_word_counts$Text
```


```{r}
select_method <- 0

col_ind <- c("Two","Three","One","Five","Zero","Four")
row_ind <- c(1:nrow(filtered_word_counts))

for (y in col_ind) {
  dummydf <- filtered_word_counts[,c("Text",y,"Total")]
  for (x in row_ind) {
    N_11 <- as.numeric(dummydf[x,2])
    N_01 <- as.numeric(sum(dummydf[-x,2])) 
    N_10 <- as.numeric(dummydf[x,3] - dummydf[x,2]) 
    N_00 <- as.numeric(sum(dummydf[-x,3]-dummydf[-x,2])) 
    
    if (select_method == 1) {
      results[x,y] <- chi2(N_11,N_01,N_10,N_00)
    }
    else {
      results[x,y] <- mutual_info(N_11,N_01,N_10,N_00)
    }
  }
}
```


```{r}
head(results)
```

```{r}
col_ind <- c("Two","Three","One","Five","Zero","Four")

wrds <- vector()

for (y in col_ind) {
  dummydf <- results[,c("Text",y)]
  
  # Select rows with results in the top A%
  top_percentage <- 0.3

  selected_rows <- dummydf %>%
  slice_max(dummydf[[2]] ,prop = top_percentage)
  rws <- as.vector(selected_rows$Text)
  wrds <- union(wrds,rws)
}
wrds <- wrds[order(-str_length(wrds))]
str(wrds)
```

Reformat original data set

```{r}
filter_words <- function(string, word_list) {
  # Split the string into individual words
  words <- strsplit(string, " ")[[1]]
  
  # Filter the words based on the match with the word list
  matched_words <- words[words %in% word_list]
  
  # Combine the matched words into a single string
  result <- paste(matched_words, collapse = " ")
  
  return(result)
}
```

Apply word filter to whole dataset?

```{r}
dftest <- data.frame(train)

# Convert everything to lowercase, remove punctuation, and split into separate words
dftest <- dftest |> mutate(Text = tolower(Text)) |>
  mutate(Text = str_replace_all(Text, "[[:punct:]]", ""))

```


```{r}
dfout <- df

for (i in c(1:nrow(dfout))) {
  #matched_words <- str_extract_all(paste(" ", dfout[i, 2], " "), paste(wrds, collapse = "|"))[[1]]
  #print(dfout[i,2])
  #matched_words <- str_extract_all(paste(" ",dfout[i,2]," "), paste(wrds, collapse = "|"))[[1]]
  dfout[i,2] <- filter_words(dfout[i,2],wrds)
  #print(dfout[i,2])
  #dfout[i,2] <- gsub("\\s+", " ", paste(dfout[i,2], collapse = " "))
  #print(dfout[i,2])
}

head(dfout[,2])
```

```{r}
head(df[,2])
```


```{r}
set.seed(69420)
test_perc <- 0.8

data <- dfout
#data <- read.csv('archive/train.csv')

length(data[,"Labels"])
data <- data[sample(1:length(data$Labels)), ] #random shuffle
length(data[,"Labels"])
val_data <- data[as.integer(test_perc * length(data$Labels)):length(data$Labels),]
data <- data[1:as.integer(test_perc * length(data$Labels)),]
length(data[,"Labels"])
length(val_data[,"Labels"])
```

```{r}
pc <- c(0,0,0,0,0,0)
for(i in 0:5) {
    pc[i+1] <- length(data[data[, "Labels"]==i, "Labels"])
}

full_text <- paste(data$Text, sep=" ", collapse=" ")
tokens <- unique(strsplit(full_text, " ")[[1]])
tokens <- tokens[- (tokens=="")]

```


### Multinomial Model

```{r}
## multinomial model
ptc <- data.frame(
    token = tokens, 
    ct0 = 0,
    ct1 = 0,
    ct2 = 0,
    ct3 = 0,
    ct4 = 0,
    ct5 = 0,
    #ctp0 = 0, 
    #ctp1 = 0, 
    #ctp2 = 0, 
    #ctp3 = 0, 
    #ctp4 = 0, 
    #ctp5 = 0, 
    pt0 = 0,
    pt1 = 0,
    pt2 = 0,
    pt3 = 0,
    pt4 = 0,
    pt5 = 0
)



for(i in 0:5) {
    class_text <- strsplit(paste(data[data[, "Labels"]==i, "Text"], sep=" ", collapse=" "), " ")[[1]]
    
    for(j in 1:length(tokens)) {
        ptc[j,paste("ct",i, sep="")] <- sum(class_text==ptc[j, "token"]) + 1
        #ptc[j,paste("ctp",i, sep="")] <- length(tokens) + length(class_text) - ptc[j,paste("ct",i, sep="")]
    }
    #ptc[,paste("pt",i, sep="")] <- ptc[,paste("ct",i, sep="")] / ptc[,paste("ctp",i, sep="")]
    ptc[,paste("pt",i, sep="")] <- ptc[,paste("ct",i, sep="")] / (length(class_text) + length(tokens))
}

ptc <- ptc[,c("token","pt0", "pt1", "pt2", "pt3", "pt4", "pt5")]
head(ptc)
```

### Testing

```{r}
right <- 0

for(k in 1:length(val_data[,"Labels"])) {
    vec <- strsplit(val_data[k, "Text"], " ")[[1]]
    probs <- log(pc)

    
    for(h in vec) {
        for(i in 0:5) {
            inc <- log(ptc[ptc[,"token"]==h ,paste("pt",i, sep="")])
            if ( length(inc) == 1 && ! any(is.na(inc)) ) {
                probs[i+1] <- probs[i+1] + inc
            }
            
        }
    }

    
    if( (0:5)[probs == max(probs)] == val_data[k, "Labels"]) {
        right <- right + 1
        
    }

}

print(100*right/length(val_data[,"Labels"]))
```


### Benchmark

```{r}
benchmark <- function(minimum_counts, percentage, information_method){
 
  Min_Counts <- vector()
  Info_Perc <- vector()
  Info_Ret <- vector()
  Accuracy <- vector() 
   
  for (mct in minimum_counts) {
    for (prc in percentage) {
      for (ifm in infmet) {
        
  # Minimum count threshold
  min_count <- mct

  # Filter out rows with counts below the threshold
  filtered_word_counts <- reformatted_word_counts %>%
  filter(Total > min_count) 
  
  
  
  # Create a Vector with Columns
  columns = c("Text","Two","Three","One","Five","Zero","Four")
  #Create a Empty DataFrame with 0 rows and n columns
  results = data.frame(matrix(nrow = nrow(filtered_word_counts), ncol = length(columns))) 
  # Assign column names
  colnames(results) = columns
  # Add word column
  results$Text <- filtered_word_counts$Text
  
  
  
  select_method <- ifm

  col_ind <- c("Two","Three","One","Five","Zero","Four")
  row_ind <- c(1:nrow(filtered_word_counts))

  for (y in col_ind) {
    dummydf <- filtered_word_counts[,c("Text",y,"Total")]
    for (x in row_ind) {
      N_11 <- as.numeric(dummydf[x,2])
      N_01 <- as.numeric(sum(dummydf[-x,2])) 
      N_10 <- as.numeric(dummydf[x,3] - dummydf[x,2]) 
      N_00 <- as.numeric(sum(dummydf[-x,3]-dummydf[-x,2])) 
    
      if (select_method == 1) {
        results[x,y] <- chi2(N_11,N_01,N_10,N_00)
        metstring <- "Chi^2"
      }
      else if (select_method == 0) {
        results[x,y] <- mutual_info(N_11,N_01,N_10,N_00)
        metstring <- "MutInf"
      }
    }
  }
  


  wrds <- vector()

  for (y in col_ind) {
    dummydf <- results[,c("Text",y)]
  
    # Select rows with results in the top A%
    top_percentage <- prc

    selected_rows <- dummydf %>%
    slice_max(dummydf[[2]] ,prop = top_percentage)
    rws <- as.vector(selected_rows$Text)
    wrds <- union(wrds,rws)
  }
  wrds <- wrds[order(-str_length(wrds))]
  
  dfout <- df

  for (i in c(1:nrow(dfout))) {
    dfout[i,2] <- filter_words(dfout[i,2],wrds)
    }
  
  
  data <- dfout

  length(data[,"Labels"])
  data <- data[sample(1:length(data$Labels)), ] #random shuffle
  length(data[,"Labels"])
  val_data <- data[as.integer(test_perc * length(data$Labels)):length(data$Labels),]
  data <- data[1:as.integer(test_perc * length(data$Labels)),]
  length(data[,"Labels"])
  length(val_data[,"Labels"])
  
  pc <- c(0,0,0,0,0,0)
  for(i in 0:5) {
      pc[i+1] <- length(data[data[, "Labels"]==i, "Labels"])
  }

  full_text <- paste(data$Text, sep=" ", collapse=" ")
  tokens <- unique(strsplit(full_text, " ")[[1]])
  tokens <- tokens[- (tokens=="")]
  
  ptc <- data.frame(
    token = tokens, 
    ct0 = 0,
    ct1 = 0,
    ct2 = 0,
    ct3 = 0,
    ct4 = 0,
    ct5 = 0,
    pt0 = 0,
    pt1 = 0,
    pt2 = 0,
    pt3 = 0,
    pt4 = 0,
    pt5 = 0
    )



  for(i in 0:5) {
      class_text <- strsplit(paste(data[data[, "Labels"]==i, "Text"], sep=" ", collapse=" "), " ")[[1]]
      
      for(j in 1:length(tokens)) {
          ptc[j,paste("ct",i, sep="")] <- sum(class_text==ptc[j, "token"]) + 1
    }
    ptc[,paste("pt",i, sep="")] <- ptc[,paste("ct",i, sep="")] / (length(class_text) + length(tokens))
  }

  ptc <- ptc[,c("token","pt0", "pt1", "pt2", "pt3", "pt4", "pt5")]

  
  
    right <- 0

  for(k in 1:length(val_data[,"Labels"])) {
      vec <- strsplit(val_data[k, "Text"], " ")[[1]]
     probs <- log(pc)

    
      for(h in vec) {
        for(i in 0:5) {
            inc <- log(ptc[ptc[,"token"]==h ,paste("pt",i, sep="")])
            if ( length(inc) == 1 && ! any(is.na(inc)) ) {
                probs[i+1] <- probs[i+1] + inc
            }
        }
    }

    
      if( (0:5)[probs == max(probs)] == val_data[k, "Labels"]) {
          right <- right + 1
        
      }

  }

  acc <- 100*right/length(val_data[,"Labels"])



  result <- paste("min_counts: ",min_count," info_percentage: ",top_percentage, "info_ret_method: ", metstring, " Accuracy%: ", acc)
  print(result)
  
  Min_Counts <- append(Min_Counts,min_count)
  Info_Perc <- append(Info_Perc,top_percentage)
  Info_Ret <- append(Info_Ret,metstring)
  Accuracy <- append(Accuracy,acc)
  
      }
    }
  }
  
  bench <- data.frame(Min_Counts,Info_Perc,Info_Ret,Accuracy)
  
  # Create a Vector with Columns
  columns = c("min_counts","info_percentage","info_ret_method","Accuracy")
  
  # Assign column names
  colnames(bench) = columns
  
  return(bench)
}
```

```{r}
min_cts <- c(1,2,5,10)
perc <- c(0.03,0.05,0.1,0.3,0.5)
infmet <- c(0,1)

bench <- benchmark(minimum_counts = min_cts,percentage = perc,information_method = infmet)
```
```{r}
(ggplot(bench[bench$info_ret_method=="Chi^2" & bench$min_counts==1,]) + geom_line(aes(info_percentage,Accuracy)) +
   scale_fill_gradient(low = "navy", high = "deepskyblue"))
```

```{r}
(ggplot(bench[bench$info_ret_method=="MutInf",]) + geom_tile(aes(min_counts,info_percentage))+
   scale_fill_gradient(low = "darkred", high = "deeppink") + theme_ipsum())
```


### Binary Classification

In order to use the alternative dataset provided we will move to a binary representation, that is
- Labels 0-1-4 are deemed "untrustworthy" (label 0)
- Labels 2-3-5 are deemed "trustworthy" (label 1)

```{r}
df <- data.frame(train)

# Convert everything to lowercase, remove punctuation, and split into separate words
df <- df |> mutate(Text = tolower(Text)) |>
  mutate(Text = str_replace_all(Text, "[[:punct:]]", ""))

# Count the occurrences of each word for each label
word_counts <- df |>
  separate_rows(Text, sep = "\\s+") |>
  group_by(Labels, Text) |>
  summarise(count = n()) |>
  #filter(!is.na(count)) |>
  arrange(-count)

word_counts[word_counts$Labels==1 | word_counts$Labels==4, "Labels"] <- 0 
word_counts[word_counts$Labels==3 | word_counts$Labels==2 | word_counts$Labels==5, "Labels"] <- 1 

# Count the occurrences of each word for each label
word_counts <- word_counts |>
  group_by(Labels, Text) |>
  summarise(count = sum(count)) |>
  #filter(!is.na(count)) |>
  arrange(-count)

# Print the word counts
print(word_counts)
```

Same models and feature selection as before, adapted of course for binary classification

```{r}
# Reformat the word_counts data frame
reformatted_word_counts <- word_counts %>%
  pivot_wider(names_from = Labels, values_from = count, values_fill = 0)%>%
  mutate(Sum = rowSums(across(-Text))) |> arrange(-Sum)

colnames(reformatted_word_counts) <- c("Text","Zero","One", "Total")

# Print the reformatted word counts
print(reformatted_word_counts)
```

```{r}
benchmark2D <- function(minimum_counts, percentage, information_method){
 
  Min_Counts <- vector()
  Info_Perc <- vector()
  Info_Ret <- vector()
  Accuracy <- vector() 
   
  for (mct in minimum_counts) {
    for (prc in percentage) {
      for (ifm in infmet) {
        
  # Minimum count threshold
  min_count <- mct

  # Filter out rows with counts below the threshold
  filtered_word_counts <- reformatted_word_counts %>%
  filter(Total > min_count) 
  
  
  
  # Create a Vector with Columns
  columns = c("Text","Zero","One", "Total")
  #Create a Empty DataFrame with 0 rows and n columns
  results = data.frame(matrix(nrow = nrow(filtered_word_counts), ncol = length(columns))) 
  # Assign column names
  colnames(results) = columns
  # Add word column
  results$Text <- filtered_word_counts$Text
  
  
  
  select_method <- ifm

  col_ind <- c("Zero","One")
  row_ind <- c(1:nrow(filtered_word_counts))

  for (y in col_ind) {
    dummydf <- filtered_word_counts[,c("Text",y,"Total")]
    for (x in row_ind) {
      N_11 <- as.numeric(dummydf[x,2])
      N_01 <- as.numeric(sum(dummydf[-x,2])) 
      N_10 <- as.numeric(dummydf[x,3] - dummydf[x,2]) 
      N_00 <- as.numeric(sum(dummydf[-x,3]-dummydf[-x,2])) 
    
      if (select_method == 1) {
        results[x,y] <- chi2(N_11,N_01,N_10,N_00)
        metstring <- "Chi^2"
      }
      else if (select_method == 0) {
        results[x,y] <- mutual_info(N_11,N_01,N_10,N_00)
        metstring <- "MutInf"
      }
    }
  }
  


  wrds <- vector()

  for (y in col_ind) {
    dummydf <- results[,c("Text",y)]
  
    # Select rows with results in the top A%
    top_percentage <- prc

    selected_rows <- dummydf %>%
    slice_max(dummydf[[2]] ,prop = top_percentage)
    rws <- as.vector(selected_rows$Text)
    wrds <- union(wrds,rws)
  }
  wrds <- wrds[order(-str_length(wrds))]
  
  dfout <- df

  for (i in c(1:nrow(dfout))) {
    dfout[i,2] <- filter_words(dfout[i,2],wrds)
    }
  
  
  data <- dfout

  length(data[,"Labels"])
  data <- data[sample(1:length(data$Labels)), ] #random shuffle
  length(data[,"Labels"])
  val_data <- data[as.integer(test_perc * length(data$Labels)):length(data$Labels),]
  data <- data[1:as.integer(test_perc * length(data$Labels)),]
  length(data[,"Labels"])
  length(val_data[,"Labels"])
  
  pc <- c(0,0)
  for(i in 0:1) {
      pc[i+1] <- length(data[data[, "Labels"]==i, "Labels"])
  }

  full_text <- paste(data$Text, sep=" ", collapse=" ")
  tokens <- unique(strsplit(full_text, " ")[[1]])
  tokens <- tokens[- (tokens=="")]
  
  ptc <- data.frame(
    token = tokens, 
    ct0 = 0,
    ct1 = 0,
    pt0 = 0,
    pt1 = 0
    )



  for(i in 0:1) {
      class_text <- strsplit(paste(data[data[, "Labels"]==i, "Text"], sep=" ", collapse=" "), " ")[[1]]
      
      for(j in 1:length(tokens)) {
          ptc[j,paste("ct",i, sep="")] <- sum(class_text==ptc[j, "token"]) + 1
    }
    ptc[,paste("pt",i, sep="")] <- ptc[,paste("ct",i, sep="")] / (length(class_text) + length(tokens))
  }

  ptc <- ptc[,c("token","pt0", "pt1")]
  
  right <- 0

  for(k in 1:length(val_data[,"Labels"])) {
      vec <- strsplit(val_data[k, "Text"], " ")[[1]]
     probs <- log(pc)

    
      for(h in vec) {
        for(i in 0:1) {
            inc <- log(ptc[ptc[,"token"]==h ,paste("pt",i, sep="")])
            if ( length(inc) == 1 && ! any(is.na(inc)) ) {
                probs[i+1] <- probs[i+1] + inc
            }
        }
    }

    
      if( (0:1)[probs == max(probs)] == val_data[k, "Labels"]) {
          right <- right + 1
        
      }

  }

  acc <- 100*right/length(val_data[,"Labels"])



  result <- paste("min_counts: ",min_count," info_percentage: ",top_percentage, "info_ret_method: ", metstring, " Accuracy%: ", acc)
  print(result)
  
  Min_Counts <- append(Min_Counts,min_count)
  Info_Perc <- append(Info_Perc,top_percentage)
  Info_Ret <- append(Info_Ret,metstring)
  Accuracy <- append(Accuracy,acc)
  
      }
    }
  }
  
  bench <- data.frame(Min_Counts,Info_Perc,Info_Ret,Accuracy)
  
  # Create a Vector with Columns
  columns = c("min_counts","info_percentage","info_ret_method","Accuracy")
  
  # Assign column names
  colnames(bench) = columns
  
  return(bench)
}
```

```{r}
min_cts <- c(1,2,5,10)
perc <- c(0.05,0.1,0.3,0.5)
infmet <- c(0,1)

bench2D <- benchmark2D(minimum_counts = min_cts,percentage = perc,information_method = infmet)
```



